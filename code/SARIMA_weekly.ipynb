{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gL6rcJanpYm","executionInfo":{"status":"ok","timestamp":1740766741636,"user_tz":300,"elapsed":613,"user":{"displayName":"Yijian Liu","userId":"16317903561391847216"}},"outputId":"aa621659-7178-405e-c870-b7fd5ba5ef38"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd drive/MyDrive/ForecastFrontiers/Electricity/data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4QiCT0eoFyx","executionInfo":{"status":"ok","timestamp":1740766778316,"user_tz":300,"elapsed":43,"user":{"displayName":"Yijian Liu","userId":"16317903561391847216"}},"outputId":"21248592-cab2-4d80-d47d-b1710567ba80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/MyDrive/ForecastFrontiers/Electricity/data'\n","/content\n"]}]},{"cell_type":"markdown","source":["# Data Preprocessing"],"metadata":{"id":"cXV0GgZ_1RgX"}},{"cell_type":"markdown","source":["## Read Data"],"metadata":{"id":"0IHsfkhh5hg7"}},{"cell_type":"markdown","source":["**Dataset Overview:**\n","\n","Dataset has **no missing values**.\n","\n","**Data time horizon**: 2011-01-01 to 2015-01-01.\n","\n","Values are in **kW of each 15 min**. To convert values in kWh values must be divided by 4.\n","\n","First column present **date and time** as a string with the following format 'yyyy-mm-dd hh:mm:ss'. For other 370 columns, each column represent **one client**. Some clients were created after 2011. In these cases consumption were considered zero.\n","\n","All time labels report to Portuguese hour. However all days present **96 measures** (24*4).\n","\n","Every year in March time change day (which has only 23 hours) the **values between 1:00 am and 2:00 am are zero for all points**. Every year in October time change day (which has 25 hours) the **values between 1:00 am and 2:00 am aggregate** the consumption of two hours.\n","\n"],"metadata":{"id":"-873fD9sRb-t"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.simplefilter(\"ignore\", category=UserWarning)"],"metadata":{"id":"XQO1v4pnqGL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\"LD2011_2014.txt\", sep=';', low_memory=False, decimal=',', parse_dates=[0], index_col=0)"],"metadata":{"id":"PiTMEFxsqBRw","colab":{"base_uri":"https://localhost:8080/","height":287},"executionInfo":{"status":"error","timestamp":1740766742728,"user_tz":300,"elapsed":174,"user":{"displayName":"Yijian Liu","userId":"16317903561391847216"}},"outputId":"ca2f4836-6a7e-4ee0-fe66-1b23073c3b81"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'LD2011_2014.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-45db1a03b42b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LD2011_2014.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LD2011_2014.txt'"]}]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"iUUW_9IDqKXq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check Data"],"metadata":{"id":"5SN7kIxm5qwf"}},{"cell_type":"code","source":["# check data type\n","df.info()"],"metadata":{"id":"rF8O_tr15FRb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check columns\n","df.columns"],"metadata":{"id":"hv4QBdSS8sda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"jXlMnw3q1WgC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check time difference\n","df.index.to_series().diff().value_counts()"],"metadata":{"id":"w_9aZVuq5b8s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Daylight saving time does not require adjustment as the dataset maintains a consistent 15-minute interval, ensuring uniform time indexing."],"metadata":{"id":"wsHZ1sDj6pZi"}},{"cell_type":"markdown","source":["## Preprocess Data"],"metadata":{"id":"naX2iElMBdYo"}},{"cell_type":"code","source":["# kW to kWh\n","df /= 4"],"metadata":{"id":"qIiN67JD5tZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# aggregate by day\n","df_daily = df.resample('D').sum()"],"metadata":{"id":"vT0FPqq17cc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_daily.to_csv('daily_usage_wide.csv')"],"metadata":{"id":"nu7sYyX3CME-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert to long table format for modeling\n","daily_usage = df_daily.melt(ignore_index=False, var_name=\"Account\", value_name=\"Usage\").reset_index().rename(columns={'index':'Datetime'})"],"metadata":{"id":"gUzzY9E7RQDp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["daily_usage.info()"],"metadata":{"id":"d1VqucCORWEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["daily_usage.head()"],"metadata":{"id":"AslwsgMsRYPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["daily_usage.to_csv('daily_usage_individual.csv')"],"metadata":{"id":"ukQYJ5aORbSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["daily_overall = daily_usage.groupby('Datetime')['Usage'].sum().reset_index()"],"metadata":{"id":"yNRa43IORcp4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["daily_overall.info()"],"metadata":{"id":"tYdoF3raRiOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["daily_overall.head()"],"metadata":{"id":"tResqc5kRkxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["daily_overall.to_csv('daily_usage_overall.csv')"],"metadata":{"id":"tmBH-ljzRoIv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# aggregate by week\n","df_weekly = df.resample('W').sum()\n","df_weekly = df_weekly.iloc[:-1]"],"metadata":{"id":"lN-KXUOn-xSW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_weekly.to_csv('weekly_usage_wide.csv')"],"metadata":{"id":"1gkHPaGpCKlW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# convert to long table format for modeling\n","weekly_usage = df_weekly.melt(ignore_index=False, var_name=\"Account\", value_name=\"Usage\").reset_index().rename(columns={'index':'Datetime'})"],"metadata":{"id":"Me9AIsj5_VWH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weekly_usage.info()"],"metadata":{"id":"1W6onZF7_XN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weekly_usage.head()"],"metadata":{"id":"FGNaLzHfCnhM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weekly_usage.to_csv('weekly_usage_individual.csv')"],"metadata":{"id":"zqc9ePLcAssw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weekly_overall = weekly_usage.groupby('Datetime')['Usage'].sum().reset_index()"],"metadata":{"id":"utfeeHUGA231"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weekly_overall.info()"],"metadata":{"id":"6K_jbEAPBBiz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weekly_overall.head()"],"metadata":{"id":"cBhH6-4RCk5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weekly_overall.to_csv('weekly_usage_overall.csv')"],"metadata":{"id":"MWDvXF9hBHNI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exploratory Data Analysis"],"metadata":{"id":"lAxIBRWfB_0m"}},{"cell_type":"code","source":["# individual account usage\n","weekly_usage = pd.read_csv(\"weekly_usage_individual.csv\")\n","weekly_usage = weekly_usage.drop(columns=['Unnamed: 0'], errors='ignore')\n","weekly_usage['Datetime'] = pd.to_datetime(weekly_usage['Datetime'])\n","# overall usage\n","weekly_overall = pd.read_csv(\"weekly_usage_overall.csv\")\n","weekly_overall = weekly_overall.drop(columns=['Unnamed: 0'], errors='ignore')\n","weekly_overall['Datetime'] = pd.to_datetime(weekly_overall['Datetime'])"],"metadata":{"id":"EH7Ngw6lCPM9","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1740766758452,"user_tz":300,"elapsed":96,"user":{"displayName":"Yijian Liu","userId":"16317903561391847216"}},"outputId":"66543063-cf90-4854-def5-e1faa3c93260"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'weekly_usage_individual.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-ae3167e7f9f9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# individual account usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweekly_usage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weekly_usage_individual.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mweekly_usage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweekly_usage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mweekly_usage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweekly_usage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# overall usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weekly_usage_individual.csv'"]}]},{"cell_type":"code","source":["# weekly_usage.info()"],"metadata":{"id":"VfJi8P5vHHK6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# weekly_overall.info()"],"metadata":{"id":"ZrgTLCVUGYzp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Overall Trend"],"metadata":{"id":"1dZYQ5NdFXrn"}},{"cell_type":"code","source":["plt.figure(figsize=(12, 5))\n","plt.plot(weekly_overall['Datetime'], weekly_overall['Usage'], marker='o', linestyle='-')\n","plt.xlabel(\"Date\")\n","plt.ylabel(\"Total Weekly Usage (kWh)\")\n","plt.title(\"Weekly Overall Electricity Usage\")\n","plt.grid()\n","plt.show()"],"metadata":{"id":"QbY2ZmFHBmRJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Moving Average"],"metadata":{"id":"R3Jiw0-MIhOp"}},{"cell_type":"code","source":["moving_avg = weekly_overall.copy()\n","moving_avg['moving_avg'] = moving_avg['Usage'].rolling(window=4, center=True).mean()\n","\n","plt.figure(figsize=(12, 5))\n","plt.plot(moving_avg.index, moving_avg['Usage'], label=\"Actual\", alpha=0.5)\n","plt.plot(moving_avg.index, moving_avg['moving_avg'], label=\"4-week Moving Average\", color='red')\n","plt.xlabel(\"Date\")\n","plt.ylabel(\"Total Weekly Usage (kWh)\")\n","plt.title(\"Moving Average of Weekly Overall Usage\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"tZ7_H01eF-yv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Autocorrelation"],"metadata":{"id":"ExFfdZQBLFE0"}},{"cell_type":"code","source":["from matplotlib.ticker import AutoMinorLocator\n","\n","plt.figure(figsize=(10, 5))\n","plt.acorr(weekly_overall['Usage'].values, maxlags=None)\n","plt.xlabel(r\"$\\tau$\")\n","plt.ylabel(\"Autocorrelation\")\n","plt.legend([r\"$R_X(\\tau)$\"])\n","plt.title(\"Autocorrelation of Weekly Electricity Usage\")\n","plt.grid()\n","plt.show()"],"metadata":{"id":"-IdCkethJu5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","\n","fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n","plot_acf(weekly_overall['Usage'], lags=200, ax=ax[0])\n","plot_pacf(weekly_overall['Usage'], lags=104, ax=ax[1])\n","plt.show()"],"metadata":{"id":"3069llrcNZDw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## PSD"],"metadata":{"id":"ekZ9nDIaI8Zw"}},{"cell_type":"code","source":["from scipy import signal\n","\n","freqs, psd = signal.welch(weekly_overall['Usage'].values, fs=1, nperseg = 210)\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(freqs, psd, marker='o', linestyle='-')\n","plt.xlabel(\"Frequency\")\n","plt.ylabel(\"Power Spectral Density\")\n","plt.legend(['PSD'])\n","plt.title(\"Power Spectral Density (PSD) of Weekly Electricity Usage\")\n","plt.grid()\n","plt.show()"],"metadata":{"id":"0Xn_KWULFaFz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Seasonal Decomposition"],"metadata":{"id":"iGEyBZpCMFQC"}},{"cell_type":"code","source":["from statsmodels.tsa.seasonal import seasonal_decompose\n","\n","result = seasonal_decompose(weekly_overall['Usage'], model='additive', period=52)\n","result.plot()\n","plt.show()"],"metadata":{"id":"U8G3TUBbF1EX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check Outliers"],"metadata":{"id":"C4NRP5F2JUDC"}},{"cell_type":"code","source":["threshold = weekly_overall['Usage'].mean() + 3 * weekly_overall['Usage'].std()\n","outliers = weekly_overall[weekly_overall['Usage'] > threshold]\n","\n","plt.figure(figsize=(12, 5))\n","plt.plot(weekly_overall.index, weekly_overall['Usage'], label=\"Usage\", alpha=0.5)\n","plt.scatter(outliers.index, outliers['Usage'], color='red', label=\"Outliers\")\n","plt.xlabel(\"Date\")\n","plt.ylabel(\"Total Weekly Usage (kWh)\")\n","plt.title(\"Anomaly Detection in Weekly Usage\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","print(outliers)"],"metadata":{"id":"hW35z_LaF4X7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model: SARIMA"],"metadata":{"id":"hekQdIRkMJOh"}},{"cell_type":"markdown","source":["1. **d = 1** (Non-seasonal Differencing)\n","\n","The original data shows a long-term increasing trend. ACF decreases gradually, indicating the need for differencing.\n","\n","2. **D = 1** (Seasonal Differencing)\n","\n","ACF at lag=52 remains significant, confirming seasonal influence.\n","\n","3. **p = 1** (Non-seasonal Autoregressive Order)\n","\n","PACF cuts off at lag=1, suggesting an AR(1) process.\n","\n","4. **q = 1** (Non-seasonal Moving Average Order)\n","\n","ACF cuts off at lag=1, indicating an MA(1) process.\n","\n","5. **P = 1** (Seasonal Autoregressive Order)\n","\n","ACF shows a peak at lag=52, confirming the need for a seasonal AR term.\n","\n","6. **Q = 1** (Seasonal Moving Average Order)\n","\n","PACF does not show a strong peak at lag=52, but selecting Q=1 ensures model robustness.\n","\n","7. **s = 52** (Seasonal Periodicity)\n","\n","The ACF plot shows a strong peak at lag=52, indicating an annual seasonal cycle. The seasonal decomposition confirms a repeating pattern every year."],"metadata":{"id":"GO-kOQiGSYJb"}},{"cell_type":"code","source":["# individual account usage\n","weekly_usage = pd.read_csv(\"weekly_usage_individual.csv\")\n","weekly_usage = weekly_usage.drop(columns=['Unnamed: 0'], errors='ignore')\n","weekly_usage['Datetime'] = pd.to_datetime(weekly_usage['Datetime'])\n","# overall usage\n","weekly_overall = pd.read_csv(\"weekly_usage_overall.csv\")\n","weekly_overall = weekly_overall.drop(columns=['Unnamed: 0'], errors='ignore')\n","weekly_overall['Datetime'] = pd.to_datetime(weekly_overall['Datetime'])"],"metadata":{"id":"kywKgPI6WDNn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.simplefilter(\"ignore\", category=UserWarning)\n","\n","from statsmodels.tsa.statespace.sarimax import SARIMAX"],"metadata":{"id":"6S-_j6xabKyO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mape(y_true, y_pred):\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"],"metadata":{"id":"dDx5W-11Wm-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train test split for overall usage\n","train_size = int(len(weekly_overall) * 0.6)\n","val_size = int(len(weekly_overall) * 0.2)\n","\n","train = weekly_overall.iloc[:train_size]\n","val = weekly_overall.iloc[train_size:train_size + val_size]\n","test = weekly_overall.iloc[train_size + val_size:]"],"metadata":{"id":"m0h1sntHWAPH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Plot-based Params"],"metadata":{"id":"mD16uz9PQVfx"}},{"cell_type":"markdown","source":["### Weekly Overall"],"metadata":{"id":"ErT1TDnE8Pbv"}},{"cell_type":"markdown","source":["#### SARIMA (1,1,1)x(1,1,1,52)"],"metadata":{"id":"SQM8LM0rtD1M"}},{"cell_type":"code","source":["# train SARIMA model\n","sarima_model = SARIMAX(train['Usage'],\n","                        order=(1,1,1),\n","                        seasonal_order=(1,1,1,52))\n","\n","sarima_result = sarima_model.fit()\n","\n","val_pred = sarima_result.predict(start=val.index[0], end=val.index[-1])\n","test_pred = sarima_result.predict(start=test.index[0], end=test.index[-1])\n","\n","val_pred.index = val.index\n","test_pred.index = test.index\n","mape_val = mape(val['Usage'], val_pred)\n","mape_test = mape(test['Usage'], test_pred)\n","\n","print(f\"Overall Validation MAPE: {mape_val:.2f}%\")\n","print(f\"Overall Test MAPE: {mape_test:.2f}%\")"],"metadata":{"id":"X2S8PWebMMNU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tISIDlWYwrtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,6))\n","\n","plt.plot(train.index, train['Usage'], label=\"Train (Overall)\", color='blue')\n","plt.plot(val.index, val['Usage'], label=\"Validation (Overall)\", color='green')\n","plt.plot(test.index, test['Usage'], label=\"Test (Overall)\", color='black')\n","\n","plt.plot(val.index, val_pred, label=\"SARIMA Validation Forecast (Overall)\", linestyle='dashed', color='orange')\n","plt.plot(test.index, test_pred, label=\"SARIMA Test Forecast (Overall)\", linestyle='dashed', color='red')\n","\n","plt.xlabel(\"Date\")\n","plt.ylabel(\"Total Weekly Usage (kWh)\")\n","plt.title(\"SARIMA Model - Overall Actual vs Forecast\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"9Ig5wvotZ3Ij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split test set into 3 equal regions\n","test_split = np.array_split(test.index, 3)\n","\n","mape_results = []\n","for i, period in enumerate(test_split):\n","    mape_period = mape(test.loc[period, 'Usage'], test_pred.loc[period])\n","    mape_results.append(mape_period)\n","    print(f\"Overall MAPE for Test Period {i+1}: {mape_period:.2f}%\")\n","\n","error_df = []\n","for i, period in enumerate(test_split):\n","    period_error = test.loc[period, 'Usage'] - test_pred.loc[period]\n","    error_df.append(pd.DataFrame({'Error': period_error, 'Test Period': f\"Overall Period {i+1}\"}))\n","\n","error_df = pd.concat(error_df)\n","\n","plt.figure(figsize=(8,5))\n","sns.boxplot(x=\"Test Period\", y=\"Error\", data=error_df)\n","plt.title(\"Overall Error Distribution Across Test Periods\")\n","plt.xlabel(\"Test Period (Overall)\")\n","plt.ylabel(\"Prediction Error\")\n","plt.grid()\n","plt.show()"],"metadata":{"id":"xzySMncBW_PY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Search-based Params"],"metadata":{"id":"k6CePoLpQc19"}},{"cell_type":"markdown","source":["### Weekly Individual Account"],"metadata":{"id":"EIUaaYYN9z1W"}},{"cell_type":"code","source":["!pip install pmdarima"],"metadata":{"id":"qlwG07xdsxlM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pmdarima import auto_arima\n","\n","auto_model = auto_arima(weekly_overall['Usage'], seasonal=True, m=52, trace=False)\n","print(auto_model.summary())\n","\n","p, d, q = auto_model.order\n","P, D, Q, s = auto_model.seasonal_order\n","print(\"Method: auto_arima\")\n","print(f\"Best p = {p}, d = {d}, q = {q}\")\n","print(f\"Best P = {P}, D = {D}, Q = {Q}, s = {s}\")"],"metadata":{"id":"RlVQHjbF0O4F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from statsmodels.tsa.stattools import adfuller, kpss\n","from statsmodels.graphics.tsaplots import plot_acf\n","\n","def check_stationarity(timeseries):\n","    adf_result = adfuller(timeseries.dropna())\n","    kpss_result = kpss(timeseries.dropna(), regression='c', nlags=\"auto\")\n","\n","    adf_stationary = adf_result[1] < 0.05  # ADF p-value < 0.05 -> Stationary\n","    kpss_stationary = kpss_result[1] > 0.05  # KPSS p-value > 0.05 -> Stationary\n","\n","    return adf_stationary and kpss_stationary\n","\n","def find_optimal_d_D(timeseries, max_d=2, max_D=2):\n","    # find d\n","    d = 0\n","    while d < max_d:\n","        print(f\"Testing d={d}...\")\n","        if check_stationarity(timeseries):\n","            print(f\"Data is stationary at d={d}\")\n","            break\n","        else:\n","            print(f\"Data is not stationary at d={d}, applying differencing...\")\n","            timeseries = timeseries.diff().dropna()\n","            d += 1\n","    else:\n","        print(\"Warning: Data is still not stationary after max_d differencing!\")\n","\n","    # find D\n","    D = 0\n","    while D < max_D:\n","        print(f\"Testing D={D}...\")\n","        # plot_acf(timeseries, lags=104)\n","        # plt.title(f\"ACF Plot for D={D}\")\n","        # plt.show()\n","        if abs(timeseries.autocorr(lag=52)) < 0.3:  # ACF(lag=52) < 0.3 -> Non-Seasonality\n","            print(f\"Data is seasonally stationary at D={D}\")\n","            break\n","        else:\n","            print(f\"Data still has strong seasonality at D={D}, applying seasonal differencing...\")\n","            timeseries = timeseries.diff(52).dropna()\n","            D += 1\n","    else:\n","        print(\"Warning: Data still shows seasonality after max_D differencing!\")\n","\n","    return d, D\n","\n","d_opt, D_opt = find_optimal_d_D(weekly_overall['Usage'])\n","print(f\"Optimal values: d={d_opt}, D={D_opt}\")"],"metadata":{"id":"2OtjE6JU3XVo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SARIMA(D=0)\n","model_D0 = SARIMAX(weekly_overall['Usage'], order=(1,1,1), seasonal_order=(1,0,1,52))\n","result_D0 = model_D0.fit()\n","test_pred_D0 = result_D0.predict(start=test.index[0], end=test.index[-1])\n","mape_D0 = mape(test['Usage'], test_pred_D0)\n","\n","# SARIMA(D=1)\n","model_D1 = SARIMAX(weekly_overall['Usage'], order=(1,1,1), seasonal_order=(1,1,1,52))\n","result_D1 = model_D1.fit()\n","test_pred_D1 = result_D1.predict(start=test.index[0], end=test.index[-1])\n","mape_D1 = mape(test['Usage'], test_pred_D1)\n","\n","# SARIMA(D=2)\n","model_D2 = SARIMAX(weekly_overall['Usage'], order=(1,1,1), seasonal_order=(1,2,1,52))\n","result_D2 = model_D2.fit()\n","test_pred_D2 = result_D2.predict(start=test.index[0], end=test.index[-1])\n","mape_D2 = mape(test['Usage'], test_pred_D2)\n","\n","print(f\"MAPE with D=0: {mape_D0:.2f}%\")\n","print(f\"MAPE with D=1: {mape_D1:.2f}%\")\n","print(f\"MAPE with D=2: {mape_D2:.2f}%\")"],"metadata":{"id":"YhLV7hQt2rgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from itertools import product\n","\n","p_values = q_values = [0, 1]\n","P_values = Q_values = [0, 1]\n","s = 52\n","d, D = 1, 1\n","\n","def grid_search_sarima(d, D):\n","    best_mape = float(\"inf\")\n","    best_params = None\n","    best_model = None\n","\n","    print(f\"\\nüîç Running Grid Search (d={d}, D={D})...\")\n","\n","    for order in product(p_values, [d], q_values):\n","        for seasonal_order in product(P_values, [D], Q_values):\n","            try:\n","                model = SARIMAX(weekly_overall['Usage'], order=order, seasonal_order=seasonal_order + (s,))\n","                result = model.fit()\n","\n","                val_pred = result.predict(start=val.index[0], end=val.index[-1])\n","                mape_val = mape(val['Usage'], val_pred)\n","\n","                if mape_val < best_mape:\n","                    best_mape = mape_val\n","                    best_params = (order, seasonal_order + (s,))\n","                    best_model = result\n","\n","            except Exception as e:\n","                print(f\"Error for {order}, {seasonal_order}: {e}\")\n","\n","    print(f\"Best Params: {best_params}\")\n","    print(f\"Validation MAPE: {best_mape:.2f}%\")\n","    return best_params, best_mape, best_model\n","\n","best_params, best_mape, best_model = grid_search_sarima(d, D)\n","\n","test_pred = best_model.predict(start=test.index[0], end=test.index[-1])\n","mape_test = mape(test['Usage'], test_pred)\n","\n","print(f\"Test MAPE: {mape_test:.2f}%\")"],"metadata":{"id":"eUWvCZfPb-2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["individual_results = {}\n","individual_accounts = weekly_usage['Account'].unique()\n","\n","for account in individual_accounts:\n","    print(f\"Applying Best SARIMA for {account}...\")\n","\n","    account_data = weekly_usage[weekly_usage['Account'] == account].set_index('Datetime')['Usage']\n","\n","    # train test split\n","    train_size = int(len(account_data) * 0.6)\n","    val_size = int(len(account_data) * 0.2)\n","\n","    train = account_data.iloc[:train_size]\n","    val = account_data.iloc[train_size:train_size + val_size]\n","    test = account_data.iloc[train_size + val_size:]\n","\n","    try:\n","        # train SARIMA model with best params\n","        model = SARIMAX(train, order=best_params[0], seasonal_order=best_params[1])\n","        result = model.fit()\n","\n","        val_pred = result.predict(start=val.index[0], end=val.index[-1])\n","        test_pred = result.predict(start=test.index[0], end=test.index[-1])\n","\n","        mape_val = mape(val, val_pred)\n","        mape_test = mape(test, test_pred)\n","\n","        individual_results[account] = {\n","            \"Validation MAPE\": mape_val,\n","            \"Test MAPE\": mape_test,\n","            \"val_pred\": val_pred,\n","            \"test_pred\": test_pred\n","        }\n","\n","        print(f\"{account}: Validation MAPE = {mape_val:.2f}%, Test MAPE = {mape_test:.2f}%\")\n","\n","    except Exception as e:\n","        print(f\"Error training SARIMA for {account}: {e}\")\n","\n","print(\"Applied Best SARIMA to All Accounts!\")"],"metadata":{"id":"ah1HkudHWgoC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","\n","# save individual SARIMA results\n","with open(\"individual_sarima_results.pkl\", \"wb\") as f:\n","    pickle.dump(individual_results, f)\n","\n","print(\"Individual SARIMA results saved!\")"],"metadata":{"id":"Xrlm_kBw6Qtr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Weekly Overall"],"metadata":{"id":"m_hmXTfnQoze"}},{"cell_type":"markdown","source":["#### SARIMA (0,1,2)x(1,0,0,52)"],"metadata":{"id":"GSFBxLgVsu-P"}},{"cell_type":"code","source":["# train SARIMA model\n","sarima_model = SARIMAX(train['Usage'],\n","                        order=(0,1,0),\n","                        seasonal_order=(1,0,0,52))\n","\n","sarima_result = sarima_model.fit()\n","\n","val_pred = sarima_result.predict(start=val.index[0], end=val.index[-1])\n","test_pred = sarima_result.predict(start=test.index[0], end=test.index[-1])\n","\n","val_pred.index = val.index\n","test_pred.index = test.index\n","mape_val = mape(val['Usage'], val_pred)\n","mape_test = mape(test['Usage'], test_pred)\n","\n","print(f\"Overall Validation MAPE: {mape_val:.2f}%\")\n","print(f\"Overall Test MAPE: {mape_test:.2f}%\")"],"metadata":{"id":"u7XWnztGsuaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,6))\n","\n","plt.plot(train.index, train['Usage'], label=\"Train (Overall)\", color='blue')\n","plt.plot(val.index, val['Usage'], label=\"Validation (Overall)\", color='green')\n","plt.plot(test.index, test['Usage'], label=\"Test (Overall)\", color='black')\n","\n","plt.plot(val.index, val_pred, label=\"SARIMA Validation Forecast (Overall)\", linestyle='dashed', color='orange')\n","plt.plot(test.index, test_pred, label=\"SARIMA Test Forecast (Overall)\", linestyle='dashed', color='red')\n","\n","plt.xlabel(\"Date\")\n","plt.ylabel(\"Total Weekly Usage (kWh)\")\n","plt.title(\"SARIMA Model - Overall Actual vs Forecast\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"tklhFeets-Az"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split test set into 3 equal regions\n","test_split = np.array_split(test.index, 3)\n","\n","mape_results = []\n","for i, period in enumerate(test_split):\n","    mape_period = mape(test.loc[period, 'Usage'], test_pred.loc[period])\n","    mape_results.append(mape_period)\n","    print(f\"Overall MAPE for Test Period {i+1}: {mape_period:.2f}%\")\n","\n","error_df = []\n","for i, period in enumerate(test_split):\n","    period_error = test.loc[period, 'Usage'] - test_pred.loc[period]\n","    error_df.append(pd.DataFrame({'Error': period_error, 'Test Period': f\"Overall Period {i+1}\"}))\n","\n","error_df = pd.concat(error_df)\n","\n","plt.figure(figsize=(8,5))\n","sns.boxplot(x=\"Test Period\", y=\"Error\", data=error_df)\n","plt.title(\"Overall Error Distribution Across Test Periods\")\n","plt.xlabel(\"Test Period (Overall)\")\n","plt.ylabel(\"Prediction Error\")\n","plt.grid()\n","plt.show()"],"metadata":{"id":"vq-HoJwEtA72"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### SARIMA (0,1,0)x(1,1,1,52)"],"metadata":{"id":"LipC1y4fs1Pn"}},{"cell_type":"code","source":["# train SARIMA model\n","sarima_model = SARIMAX(train['Usage'],\n","                        order=(0,1,0),\n","                        seasonal_order=(1,1,1,52))\n","\n","sarima_result = sarima_model.fit()\n","\n","val_pred = sarima_result.predict(start=val.index[0], end=val.index[-1])\n","test_pred = sarima_result.predict(start=test.index[0], end=test.index[-1])\n","\n","val_pred.index = val.index\n","test_pred.index = test.index\n","mape_val = mape(val['Usage'], val_pred)\n","mape_test = mape(test['Usage'], test_pred)\n","\n","print(f\"Overall Validation MAPE: {mape_val:.2f}%\")\n","print(f\"Overall Test MAPE: {mape_test:.2f}%\")"],"metadata":{"id":"ZHZpbOk0Q9_x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12,6))\n","\n","plt.plot(train.index, train['Usage'], label=\"Train (Overall)\", color='blue')\n","plt.plot(val.index, val['Usage'], label=\"Validation (Overall)\", color='green')\n","plt.plot(test.index, test['Usage'], label=\"Test (Overall)\", color='black')\n","\n","plt.plot(val.index, val_pred, label=\"SARIMA Validation Forecast (Overall)\", linestyle='dashed', color='orange')\n","plt.plot(test.index, test_pred, label=\"SARIMA Test Forecast (Overall)\", linestyle='dashed', color='red')\n","\n","plt.xlabel(\"Date\")\n","plt.ylabel(\"Total Weekly Usage (kWh)\")\n","plt.title(\"SARIMA Model - Overall Actual vs Forecast\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"LHV4vTg-Q2Hx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# split test set into 3 equal regions\n","test_split = np.array_split(test.index, 3)\n","\n","mape_results = []\n","for i, period in enumerate(test_split):\n","    mape_period = mape(test.loc[period, 'Usage'], test_pred.loc[period])\n","    mape_results.append(mape_period)\n","    print(f\"Overall MAPE for Test Period {i+1}: {mape_period:.2f}%\")\n","\n","error_df = []\n","for i, period in enumerate(test_split):\n","    period_error = test.loc[period, 'Usage'] - test_pred.loc[period]\n","    error_df.append(pd.DataFrame({'Error': period_error, 'Test Period': f\"Overall Period {i+1}\"}))\n","\n","error_df = pd.concat(error_df)\n","\n","plt.figure(figsize=(8,5))\n","sns.boxplot(x=\"Test Period\", y=\"Error\", data=error_df)\n","plt.title(\"Overall Error Distribution Across Test Periods\")\n","plt.xlabel(\"Test Period (Overall)\")\n","plt.ylabel(\"Prediction Error\")\n","plt.grid()\n","plt.show()"],"metadata":{"id":"7XiyvK56Q6N4"},"execution_count":null,"outputs":[]}]}